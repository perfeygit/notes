[[_TOC_]]

# python基础

[复习一](https://www.cnblogs.com/perfey/p/9986940.html)
[复习二](https://www.cnblogs.com/perfey/p/10223485.html)

## 装饰器

````python
带参数的装饰器
def counter(num):
	def wrapper(func):
		def inner(*args,**kwargs):
			for i in range(num):
				ret = func(*args,**kwargs)
			return ret # 只返回最后一次的结果
		return inner
	return wrapper


@counter(3)
def func():
	print(111)
````

在不改变装饰器的情况下,给装饰器的执行加上log信息
给装饰器加一层装饰器

````python
from functools import wraps
 
from logger.log import get_logger
import traceback
def decoratore(func):
    @wraps(func)
    def log(*args,**kwargs):
        try:
            print("当前运行方法",func.__name__)
            return func(*args,**kwargs)
        except Exception as e:
            get_logger().error(f"{func.__name__} is error,here are details:{traceback.format_exc()}")
    return log
````



## 生成器

### yield

推荐博客:https://blog.csdn.net/soonfly/article/details/78361819
yield具有return的功能,只是yield是中断函数(更准确的说是生成器),等待下一个next()或send()再继续执行至下一个yield
协程就是利用一个cpu运行一个线程,通过yield分时段执行多个任务,即当执行的任务遇到IO阻塞等待时,cpu就不执行这个任务转而顺序执行下一个任务


```python
def gen():
    value=0
    while True:
        receive=yield value
        if receive=='e':
            break
        value = 'got: %s' % receive

g=gen()
print(g.send(None))   
print(g.send('hello'))
print(g.send(123456))
print(g.send('e'))
```
其实receive=yield value包含了3个步骤：
1、向函数外抛出（返回）value
2、暂停(pause)，等待next()或send()恢复
3、赋值receive=MockGetValue() 。 这个MockGetValue()是假想函数，用来接收send()发送进来的值

1、通过g.send(None)或者next(g)启动生成器函数，并执行到第一个yield语句结束的位置
​    运行receive=yield value语句时，我们按照开始说的拆开来看，实际程序只执行了1，2两步，程序返回了value值，并暂停(pause)，并没有执行第3步给receive赋值。因此yield value会输出初始值0
2、通过g.send('hello')，会传入hello，从上次暂停的位置继续执行，那么就是运行第3步，赋值给receive。然后计算出value的值，并回到while头部，遇到yield value，程序再次执行了1，2两步，程序返回了value值，并暂停(pause)。此时yield value会输出”got: hello”，并等待send()激活
3、通过g.send(123456)，会重复第2步，最后输出结果为”got: 123456″。
4、当我们g.send(‘e’)时，程序会执行break然后推出循环，最后整个函数执行完毕，所以会得到StopIteration异常。

<p style="color: red">yield中的send() 先传值(将参数传给yield等号左边的变量),再执行程序抛出返回值,因此第一次不能传值,必选先使用next()或者send(None)方法</p>

```python
#send 获取下一个值的效果和next基本一致
#只是在获取下一个值的时候，给上一yield的位置传递一个数据
#使用send的注意事项
    ``# 第一次使用生成器的时候 是用next获取下一个值
    ``# 最后一个yield不能接受外部的值,因为接收了之后没有下一个yield可以抛出返回值
yield from iterable本质上等于for item in iterable: yield item的缩写版,对可迭代对象的元素逐一yield
```



### asyncio

asyncio是一个基于事件循环的实现异步I/O的模块。通过yield from，我们可以将协程asyncio.sleep的控制权交给事件循环，然后挂起当前协程；之后，由事件循环决定何时唤醒asyncio.sleep,接着向后执行代码。
协程之间的调度都是由事件循环决定。

````python
import asyncio,random
@asyncio.coroutine
def smart_fib(n):
    index = 0
    a = 0
    b = 1
    while index < n:
        sleep_secs = random.uniform(0, 0.2)
        yield from asyncio.sleep(sleep_secs) #通常yield from后都是接的耗时操作
        print('Smart one think {} secs to get {}'.format(sleep_secs, b))
        a, b = b, a + b
        index += 1
 
@asyncio.coroutine
def stupid_fib(n):
    index = 0
    a = 0
    b = 1
    while index < n:
        sleep_secs = random.uniform(0, 0.4)
        yield from asyncio.sleep(sleep_secs) #通常yield from后都是接的耗时操作
        print('Stupid one think {} secs to get {}'.format(sleep_secs, b))
        a, b = b, a + b
        index += 1
 
if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    tasks = [
        smart_fib(10),
        stupid_fib(10),
    ]
    loop.run_until_complete(asyncio.wait(tasks))
    print('All fib finished.')
    loop.close()
#yield from语法可以让我们方便地调用另一个generator。
#本例中yield from后面接的asyncio.sleep()是一个coroutine协程(里面也用了yield from)，所以线程不会等待asyncio.sleep()，而是直接中断并执行下一个消息循环。当asyncio.sleep()返回时，线程就可以从yield from拿到返回值（此处是None），然后接着执行下一行语句。


````

````python
async和await
弄清楚了asyncio.coroutine和yield from之后，在Python3.5中引入的async和await就不难理解了：可以将他们理解成asyncio.coroutine/yield from的完美替身。当然，从Python设计的角度来说，async/await让协程表面上独立于生成器而存在，将细节都隐藏于asyncio模块之下，语法更清晰明了
加入新的关键字 async ，可以将任何一个普通函数变成协程
import time,asyncio,random
async def mygen(alist):
    while len(alist) > 0:
        c = randint(0, len(alist)-1)
        print(alist.pop(c))
a = ["aa","bb","cc"]
c=mygen(a)
print(c)
输出：
<coroutine object mygen at 0x02C6BED0>
 
在上面程序中，我们在前面加上async，该函数就变成一个协程了。
但是async对生成器是无效的。async无法将一个生成器转换成协程。 (因此我们上面的函数使用的是print,不是yield)
 
要运行协程，要用事件循环
import time,asyncio,random
async def mygen(alist):
    while len(alist) > 0:
        c = random.randint(0, len(alist)-1)
        print(alist.pop(c))
        await asyncio.sleep(1)
strlist = ["ss","dd","gg"]
intlist=[1,2,5,6]
c1=mygen(strlist)
c2=mygen(intlist)
print(c1)
if __name__ == '__main__':
        loop = asyncio.get_event_loop()
        tasks = [
            c1,
            c2
        ]
        loop.run_until_complete(asyncio.wait(tasks))
        print('All fib finished.')
        loop.close()
````

### 基于yield并发执行的协程

````python
import time
def consumer():
    '''任务1:接收数据,处理数据'''
    while True:
        print('我是consumer生成器,我在接收数据')
        x=yield
 
def producer():
    '''任务2:生产数据'''
    g=consumer()
    next(g)
    for i in range(100):
        g.send(i)  # 通过send() 或者next()控制生成器consumer是否运行,用这个特性来分时段执行多个任务
        # 我们可以在这个函数中管理多个生成器(对应多个任务),然后通过一些条件(比如说for i in range() 如果i对3取余,余数是0执行任务1,余数是1或2执行任务2),来模拟各个任务的执行时间的不同,来控制任务的执行
        print(f'我是producer,我给生成器发送数据,数据是{i}')
 
start=time.time()
#基于yield保存状态,实现两个任务直接来回切换,即并发的效果
#PS:如果每个任务中都加上打印,那么明显地看到两个任务的打印是你一次我一次,即并发执行的.
producer()
 
stop=time.time()
print(stop-start)
````



可迭代对象  内部含有__iter__方法  str,list,tuple,dict,set,range
迭代器     内部含有__iter__方法,并且含有__next__方法  f文件句柄,map,filter,zip,等,生成器本身就是迭代器
迭代器的特点:1,节省内存.2,惰性机制3,单向不可逆
python内置的next()方法实际上就是执行函数的__next__方法
it = iter([1, 2, 3, 4, 5])   # iter将可迭代对象变为迭代器,也可以在对象的类中加入__next__方法





## 递归



# 数据库

## mysql

- 创建数据库

  ```python
  import pymysql
  db = pymysql.connect(host='localhost',user='root',password='123',port=3306)
  cursor = db.cursor()
  # cursor.execute('select version()')  # 查询版本
  # data = cursor.fetchone()  # fetchone() 获取第一条数据;fetchall() 获取所有数据
  # print(data)  
  cursor.execute('create database spiders default character set utf8')  # 创建spiders数据库
  db.close()
  ```

- 创建表

  ```python
  import pymysql
  db = pymysql.connect(host='localhost',user='root',password='123',port=3306,db='spiders')
  cursor = db.cursor()
  # cursor.execute('use spiders') 如果在连接的时候没有指定数据库,可以在这里指定,每个execute中只能写一句sql语句
  cursor.execute('create table if not exists students (id varchar(255) not null,name varchar(255) not null,age int not null,primary key(id))')
  cursor.execute('show tables')
  print(cursor.fetchall())
  db.close()
  ```

- 插入数据

  ```python
  import pymysql
  db = pymysql.connect(host='localhost',user='root',password='123',port=3306,db='spiders')
  cursor = db.cursor()
  id,user,age = '200010','Bob',20
  sql = 'insert into studens(id,name,age) values (%s,%s,%s)'
  try:
      cursor.execute(sql,(id,user,age))
      db.commit()   # 插入数据要提交,不然不会保存在数据库
  except:
      db.rollback()  # 如果执行失败,调用rollback()进行数据回滚,相当于什么都没发生过
  db.close()
  # 这是事务操作,原子性、一致性、隔离性、持久性
  
  # 字典形式的sql语句构造
  data = {
      'id':200010,
      'name':'Bob',
      'age':20
  }
  table = 'students'
  keys = ','.join(data.keys())
  values = ','.join(['%s']*len(data))
  sql = 'insert into {table}({keys}) values({values})'.format(table=table,keys=keys,values=values)
  cursor.execute(sql,tuple(data.values()))
  ```

- 更新数据

  ```python
  import pymysql
  db = pymysql.connect(host='localhost',user='root',password='123',port=3306,db='spiders')
  cursor = db.cursor()
  
  sql = 'update students set age=%s where name = %s'
  try:
      cursor.execute(sql,(25,'Bob'))
      db.commit()
  except:
      db.rollback()
  db.close()
  
  # 在爬取数据时我们经常使用的操作是如果数据存在则更新,不存在则插入
  data = {
      'id':5,
      'name':'Bob',
      'age':22
  }
  table = 'students'
  keys = ','.join(data.keys())
  values = ','.join(['%s']*len(data))
  sql = 'insert into {table}({keys}) values({values}) on duplicate key update '.format(table=table,keys=keys,values=values)
  update = ','.join(["{key}=%s".format(key=key) for key in data])
  sql += update
  print(sql,tuple(data.values())*2)  #实际执行的sql语句是 insert into students(id,name,age) values(%s,%s,%s) on duplicate key update id=%s,name=%s,age=%s (5, 'Bob', 20, 5, 'Bob', 20)
  try:
      if cursor.execute(sql,tuple(data.values())*2):
          print('Successful')
          db.commit()
  except:
      print('Failed')
      db.rollback()
  db.close()
  
  ```

- 删除数据

  ```python
  import pymysql
  db = pymysql.connect(host='localhost',user='root',password='123',port=3306,db='spiders')
  cursor = db.cursor()
  table='students'
  condition = 'age > 20'
  sql = 'delete from {table} where {condition}'.format(table=table ,condition=condition)
  try:
      cursor.execute(sql)
      db.commit()
  except:
      db.rollback()
  db.close()
  
  
  ```

- 查询数据

  ```python
  sql = 'select * from table where ...'
  # 基本上也是通过原生sql进行查询
  ```

[mysql查询的详细用法](https://www.cnblogs.com/perfey/p/9902215.html)
[原生mysql](https://www.cnblogs.com/perfey/p/10432844.html)



## redis

- 开启服务端,在cmd中输入redis-server
- 开启客户端,在cmd中输入redis-cli

[redis博客](https://www.cnblogs.com/perfey/p/9970984.html)
[引用博客](http://www.cnblogs.com/wupeiqi/articles/5132791.html)

````python
import redis

r = redis.Redis(host='127.0.0.1', port=6379)


# print(r.keys("*"))
# print(r.llen('data'))  # llen()  列表类型的数量
# print(r.type("data"))
# print(r.lpop('data').decode('utf-8'))
def list_iter(name):
    """
    自定义redis列表增量迭代
    :param name: redis中的name，即：迭代name对应的列表
    :return: yield 返回 列表元素
    """
    list_count = r.llen(name)
    for index in range(list_count):
        yield r.lindex(name, index)


for key in r.keys("*"):
    print("类型:"+ r.type(key).decode(),"键:"+ key.decode())
    if r.type(key) == b'string':
        print('字符串',r.get(key))
    if r.type(key) == b'list':
        print('列表数量 '+key.decode()+':'+str(r.llen(key)))
        for item in list_iter(key):
            print('列表',item.decode())
    if r.type(key) == b'hash':
        print('字典数量 ' + key.decode() + ':' + str(r.hlen(key)),'字典键:',r.hkeys(key))
        # print(r.hget(key,'age'))    # 获取键对应的值
        # print('字典',r.hgetall(key))  # 一下获取字典所有键值对(可能会撑爆内存)
        for item in r.hscan_iter(key):
            print('字典',item)
    if r.type(key) == b'zset':
        print('有序集合数量:',r.zcard(key))
        for item in r.zscan_iter(key):
            print('有序集合',item)

````



​	

## mongodb
- 开启服务端,cmd中输入mongod

- 开启客户端,cmd中输入mong.exe

[详细用法参见博客](https://www.cnblogs.com/perfey/p/10038551.html)

   <center>功能符号</center>

| 符号    | 含义           | 示例                                           | 示例含义                       |
| ------- | -------------- | ---------------------------------------------- | ------------------------------ |
| $regex  | 匹配正则表达式 | {'name':{'$regex':'^M.*'}}                     | name以M开头                    |
| $exists | 属性是否存在   | {'name':{'$exists':True}}                      | name属性存在                   |
| $type   | 类型判断       | {'age':{'$type':'int'}}                        | age的类型为int                 |
| $mod    | 数字模操作     | {'age':{'$mod':[5,0]}}                         | 年龄对5取模余0,5的整数倍       |
| $text   | 文本查询       | {'$text':{'$search':'Mike'}}                   | text类型的属性中包含Mike字符串 |
| $where  | 高级条件查询   | {'$where':'obj.fans_count==obj.follows_count'} | 自身粉丝数等于关注数           |
计数	count
count = collection.find().count()
print(count)

student = collection.find_one({'name':'Kevin'})
student['age'] = 26
result = collection.update_one({'name':'Kevin',{'$set':student}})
print(result.match_count,result.modified_count) # 匹配的数据条数,受影响的数据条数

## RabbitMQ、SQLAlchemy

[推荐博客](https://www.cnblogs.com/wupeiqi/articles/5132791.html)


# requests的参数

```python
import requests

data = {
    'name':'perfey',
    'age':'18'
}
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
    'cookie': 'd_c0="AIAkb1Qu3w2PTvKo0YXcKkNxgdk5LL_dzFI=|1531095721"; _zap=88a080ea-b5eb-49fd-bdbc-84f6b0d81cff; _xsrf=y5QFxGkZVe1KU2BchUd32iN3fpWyhCEN'
}
proxies = {
    'http':'http://10.10.1.10:3128',
    'https':'http://10.10.1.10:1080'
}
response = requests.get('www.baidu.com',params=data,headers=headers,verify=False,proxies=proxies,timeout=1,auth=('username','password'))  # params是参数,相当于访问的是www.baidu.com?name=perfey&age=18 verify=False设置不进行证书验证 proxies设置代理ip timeout超时设置,超时抛出异常 auth身份认证,对于一些需要用户名和密码的网站需要使用
print(response.text)  #字符串
print(response.content) #字节
print(response.json()) #json格式的响应
print(response.status_code) #获取响应状态
print(response.headers)  #获取响应头
print(response.cookies) #获取cookies

# post请求
request.post('www.baidu.com',data=data)  # 这里的data是请求体的内容

files = {'file':open('faicon.ico','rb')}
request.post('http://httpbin.org/post',files=files) # 上传文件

# 会话维持,自动保存cookies并使用这个cookies
import requests
s = request.Session()
s.get('http://httpbin.org/cookies/set/number/1234556789')
r = s.get('http://httpbin.org/cookies')
print(t.text)

```

# 解析库的使用

## xpath

- nodename 选取此节点的所有子节点
- /     从此节点选取直接子节点
- //   从此节点选取子孙节点
- .     选取当前节点
- ..    选取当前节点的父节点
- @   选取属性
- text() 获取字符串
- |  计算两个节点集,
- or	 and  	mod(取余) 	 +	 - 	* 	div(除)  	= 	  !=    	> 	< 
- 按序选择
  - xpath('//li[1]/a/text()')   第一个li节点
  - xpath('//li[last()]/a/text()')  最后一个li节点
  - xpath('//li[position()<3]/a/text()')   位置小于3的节点,也就是位置序号为1和2的节点
  - xpath('//li[last()-2]/a/text()')   倒数第三个节点
- 节点轴选择
  - xpath('//li[1]/ancestor::*)  获取所有祖先节点,包括父节点,爷爷节点等
  - xpath('//li[1]/ancestor::div') 只获取当前节点的div祖先节点
  - xpath('//li[1]/attribute::*')  获取当前节点的所有属性
  - xpath('//li[1]/child::a[@href='link1.html']') 获取当前节点href属性为link1.html的所有直接子节点
  - xpath('//li[1]/descendant::span')  获取当前节点的所有span子孙节点
  - xpath('//li[1]/following::*[2]')  follwing可以获取当前节点之后的所有节点,*[2] 进行索引选择,获取第二个后续节点
  - xpath('//li[1]/following-sibling'::*)  获取当前节点之后的所有同级节点 
```python
from lxml import etree
# 解析字符串
tree = etree.HTML(text)    # 将requests请求得到的字符串转化为一个etree对象
# 解析html文件
parse = etree.HTMLParser(encoding="utf-8")
tree = etree.parse("ip.html", parser=parser)  # 将html文档或者xml文档转换成一个etree对象

result = etree.tostring(tree) #将etree对象转化为字节
print(result.decode('utf-8')) #将字节解码成字符串

tree.xpath('//li[@class="item-0"]')   #使用xpath解析etree对象,并返回列表

```





## Beautiful Soup

###  属性选择,父子节点
```python
from bs4 import BeautifulSoup
soup = BeautifulSoup('<p>hello</p>','lxml')    # 将字符串转化为BeautifulSoup对象(不标准的html字符串会转化为标准的)
print(soup.p.string)  #打印p节点的字符串
print(soup.prettify())  # 将要解析的字符串以标准的缩进格式输出(方便看)

soup.p # 这种提取方式是按标签进行提取,获得的是第一个p标签

# 获取名称
soup.p.name  # 输出是p ,name属性可以获取节点的名称
# 获取属性
soup.p.attrs # {'class':['title'],'name':'dromouse'}  得到节点元素的所有属性,以字典的形式返回
soup.p.attrs['name'] # 输出 dromouse ,只获取name属性,这也是字典取值的方法
soup.p['name'] # 简化写法,直接获取p节点的name属性,注意返回值可能是字符串也可能是列表
# 获取内容
soup.p.string # 获取第一个p节点里面的文本

# 获取直接子节点
soup.p.contents # 获取p节点下面的所有直接子节点(孙节点包含在子节点中),包括文本和标签,以列表的形式返回
soup.p.children # 也是获取直接子节点,和contents不同的是,这个的返回值是个生成器,需要以for循环的形式输出子节点内容
# 获取所有子孙节点
soup.p.descendants # 以生成器的形式返回所有子孙节点

# 获取父节点
soup.a.parent # 获取a标签的直接父节点
soup.a.parents # 获取a标签的所有祖先节点(父节点,爷爷节点) ,以生成器的方式返回

# 兄弟节点
soup.a.next_sibling # 获取节点的下一个兄弟元素(注意文本也是元素)
soup.a.previous_sibling # 获取节点的上一个兄弟元素
soup.a.next_siblings # 获取节点后面所有的兄弟元素,返回值是生成器
soup.a.previous_siblings # 获取节点前面所有的兄弟元素,返回值是生成器



```

###  方法选择器

- find_all()  查询所有符合条件的元素,给它传入一些属性或文本,以列表的形式返回符合条件的元素
  - name  根据标签名称来查询元素,如soup.find_all(name='li') 找到所有li标签(而soup.li只能找到第一个li标签)
  - attrs 根据属性来查询
    - soup.find_all(attrs={'id':'list-1'}) 查找所有id为list-1的节点
    - soup.fiind_all(attrs={'class'='element'})  查找所有class属性为element的节点
    - soup.find_all(id='list-1')  对于id,class等一些常用的属性可以直接写,因为class是Python中的关键字,所以需要在class的后面加个下划线来区分开
    - soup.find_all(class_='element')
   - text 根据本文内容匹配节点,可以传入字符串,也可以传入正则表达式
      - soup.find_all(text=re.compile('link'))
- find() 用法同find_all(),只是返回的是第一个匹配的元素
- 以下的用法全部同find_all() 只是查询范围不同,
- find_parents()   find_parent()   前者返回当前节点的所有祖先节点,括号中的是筛选条件,后者返回直接父节点,
- find_next_siblings() find_next_sibling()  前者返回后面所有兄弟节点,后者返回后面的第一个兄弟节点
- find_previous_siblings() find_previous_sibling() 前者返回前面所有兄弟节点,后者返回前面的第一个兄弟节点
- find_all_next()  find_next()   前者返回当前节点后所有符合条件的节点,后者返回第一个符合条件的节点
- find_all_previous()  find_previous()   前者返回当前节点前所有符合条件的节点,后者返回第一个符合条件的节点
### css选择器

- soup.select(' ul li ')  以css的作为选择条件,返回值是一个列表
- soup.select('#list-2 .element')[0].select('li')  因为返回类型都是tag类型,所以可以嵌套使用,前面的几种选择器的返回类型也是tag类型,均可互相嵌套

注意tag类型的相互嵌套时,不能选择最外层节点,是找不到的.如s = '<p>aaaaa</p>' s是个tag类型的,s.select('p') s.p 这样得到的结果都是空

## pyquery

### 初始化

```python
from pyquery import PyQuery as pq
doc = pq(str)  # 字符串初始化,str是一段html字符串
print(doc('li'))  # 通过css选择器获取html字符串中的所有li标签,返回值是所有标签组成的html字符串,并不是列表

doc = pq(url='http://www.baidu.com')  #url传递进行初始化,相当于先请求url在把返回的字符串作为参数传递
doc = pq(filename='demo.html')  #文件初始化,读取本地html文件

```

### 基本css选择器

```python
print(doc('#container .list li'))  # 先选取id为container的节点,然后再选取其内部的class为list的节点内部的所有li节点,然后打印输出
```

### 查找节点

以下的方法均是PyQuery类型变量的方法,即亦可嵌套使用

- find('li')  查找当前节点的所有是li的子孙节点
- children('.active')  查找子节点中class为active的节点
- parent() 获取某个节点的父节点
- parents()  获取所有祖先节点,然后可以使用css进行筛选
- siblings() 获取当前节点的所有兄弟节点(不包括自己),也是可以在括号中使用css进行筛选
- items()  此方法可以将查询出来的节点变为生成器,然后使用for循环遍历就可以得到每个节点了

### 获取信息

- 获取属性

   ​	a = doc('.item-0.active a')     

    	a.attr('href')  或 a.attr.href      # 获取a节点的href属性,如果有多个a节点这里只能获取第一个节点的属性,如果想要获取每个a节点的属性就要使用items()然后遍历 ,如下

   ​	for item in a.items():

   ​		print(item.attr('href'))

- 获取文本
   ​	
   ```python
   li = doc('.item-0 .active')
   li.html()  # 获取第一个li节点的html文本
   li.text()  # 返回所有li节点内部的纯文本,不同li节点的文本使用空格分隔开,即返回结果是一个字符串,不需要遍历
   ```

### 节点操作  

- addClass()    removeClass()   添加/移除节点属性,可以对节点的class属性进行添加和删除,如li.remove('active')  li.addClass('active')  就是分别给li标签移除/添加active属性
- attr()   text()  html()  更改属性值、文本内容、html文本内容
  li.attr('name','link')  li标签添加一个属性名为name,属性值为link的属性
  li.text('aaaaa') 将li标签的文本改为aaaaa
  li.html('<span>qqqqq</span>') 将li标签内部改为span标签
- remove() 移除节点,对于一些干扰我们提取信息的节点可以使用此方法进行移除
  doc('.wrap').find('p').remove()
### 伪类选择器

- doc('li:first-child')  选择第一个节点
- doc('li:last-child')  选择最后一个节点
- doc('li:nth-child(2)')  选择第二个节点
- doc('li:gt(2)')  选择大于第二个节点的节点,即第三个及以后的节点
- doc('li:nth-child(2n)')  偶数位置的li节点
- doc('li:contains(aaaa)')  包含aaaa文本的li节点



# selenium的使用

````python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By  #按照什么方式查找，By.ID,By.CSS_SELECTOR
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import  WebDriverWait  #等待页面加载某些元素

# 无头浏览器的配置
chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')

browser = webdriver.Chrome(chrome_options=chrome_options)  #无头浏览器添加chrome_options参数,有界面的不添加

try:
    browser.get('https://www.baidu.com')

    input_tag=browser.find_element_by_id('kw')
    input_tag.send_keys('美女') 
    input_tag.send_keys(Keys.ENTER) #输入回车

    wait=WebDriverWait(browser,10)
    wait.until(EC.presence_of_element_located((By.ID,'content_left'))) #等到id为content_left的元素加载完毕,最多等10秒

    print(browser.page_source)  # 网站源代码
    print(browser.current_url)  # 当前的url
    print(browser.get_cookies()) # 当前的cookies

finally:
    browser.close()


````

## 获取单个节点的方法

```python
browser.find_element_by_id('q') # 通过id查找
browser.find_element_by_name()  # 通过name属性查找
browser.find_element_by_xpath('//*[@id="q"]') # xpath选择器
browser.find_element_by_link_text()   # 通过链接文字查找
browser.find_element_by_partial_link_text()  # 通过局部链接文字查找
browser.find_element_by_tag_name()  # 标签名查找
browser.find_element_by_class_name() # 类名查找
browser.find_element_by_css_selector('#q')  # css选择器
```

### 强调
1,上述均可以改写成find_element(By.ID,'kw')的形式
2,find_elements_by_xxx的形式是查找到多个元素，结果为列表

```python
button=wait.until(EC.element_to_be_clickable((By.CLASS_NAME,'tang-pass-footerBarULogin')))  # 等待可被点击,显式等待
input_user=wait.until(EC.presence_of_element_located((By.NAME,'userName')))  # 等待加载出来
tag=browser.find_element(By.CSS_SELECTOR,'#cc-lm-tcgShowImgContainer img')
#  隐式等待时看DOM中有无节点,如果没有则等待规定的时间然后再次查看,如果还是没有则抛出异常
#  显式等待是看DOM中有无节点,规定了一个最大等待时间,在这个时间内如果节点没有出现则抛出异常,出现则返回,推荐使用显式等待

#获取标签属性，
print(tag.get_attribute('src'))
#获取标签ID，位置，名称，大小（了解）
print(tag.id)
print(tag.location)
print(tag.tag_name)
print(tag.size)

input_tag=browser.find_element_by_id('twotabsearchtextbox')
input_tag.clear() #清空输入框
input_tag.send_keys('iphone7plus')
button=browser.find_element_by_css_selector('#nav-search > form > div.nav-right > div > input')
button.click()
```

## 节点交互

### 动作链

```python
from selenium import webdriver
from selenium.webdriver import ActionChains
from selenium.webdriver.common.by import By  # 按照什么方式查找，By.ID,By.CSS_SELECTOR
from selenium.webdriver.common.keys import Keys  # 键盘按键操作
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait  # 等待页面加载某些元素
import time

driver = webdriver.Chrome()
driver.get('http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable')
wait=WebDriverWait(driver,3)
# driver.implicitly_wait(3)  # 使用隐式等待

try:
    driver.switch_to.frame('iframeResult') ##切换到iframeResult
    sourse=driver.find_element_by_id('draggable')
    target=driver.find_element_by_id('droppable')

    #方式一：基于同一个动作链串行执行
    actions=ActionChains(driver) #拿到动作链对象
    actions.drag_and_drop(sourse,target) #把动作放到动作链中，准备串行执行
    actions.perform()

    #方式二：不同的动作链，每次移动的位移都不同
    ActionChains(driver).click_and_hold(sourse).perform()
    distance=target.location['x']-sourse.location['x']

    track=0
    while track < distance:
        ActionChains(driver).move_by_offset(xoffset=2,yoffset=0).perform()
        track+=2
        time.sleep(0.5)

    ActionChains(driver).release().perform()
    
    time.sleep(10)
finally:
    driver.close()
```

### 自己写js

在交互比较难是可以自己写js

````python
try:
    browser=webdriver.Chrome()
    browser.get('https://www.baidu.com')
    browser.execute_script('alert("hello world")') #打印警告  execute_script()这个函数可以执行javascript命令
finally:
    pass
    # browser.close()
````

### frame的切换

```python
#frame相当于一个单独的网页，在父frame里是无法直接查看到子frame的元素的，必须switch_to_frame切到该frame下，才能进一步查找
try:
    browser=webdriver.Chrome()
    browser.get('http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable')

    browser.switch_to.frame('iframeResult') #切换到id为iframeResult的frame

    tag1=browser.find_element_by_id('droppable')
    print(tag1)

    # tag2=browser.find_element_by_id('textareaCode') #报错，在子frame里无法查看到父frame的元素
    browser.switch_to.parent_frame() #切回父frame,就可以查找到了
    tag2=browser.find_element_by_id('textareaCode')  # 这是那个代码的输入框
    print(tag2)

finally:
    browser.close()
```

### 模拟浏览器的前进后退

```python
import time
from selenium import webdriver

browser=webdriver.Chrome()
browser.get('https://www.baidu.com')
browser.get('https://www.taobao.com')
browser.get('http://www.sina.com.cn/')

browser.back()   # 后退
time.sleep(10)
browser.forward() #前进
browser.close()
```

### cookies

```python
from selenium import webdriver

browser=webdriver.Chrome()
browser.get('https://www.zhihu.com/explore')
print(browser.get_cookies())  # 获取所有的cookies,是个列表,列表里套字典
browser.add_cookie({'k1':'xxx','k2':'yyy'})  # 添加cookies
print(browser.get_cookies()) 

# browser.delete_all_cookies()  # 删除所有的cookies
```

### 选项卡管理

```python
import time
from selenium import webdriver

browser=webdriver.Chrome()
browser.get('https://www.baidu.com')
browser.execute_script('window.open()')

print(browser.window_handles) #获取所有的选项卡
# 横线表示可以运行,但是pyharm不建议使用这种方法,有更好的方法替代  使用 switch_to.window  代替 switch_to_window
# 参见博客 https://blog.csdn.net/ccggaag/article/details/76652274
browser.switch_to_window(browser.window_handles[1])  # 在第二个选项卡打开淘宝页面
browser.get('https://www.taobao.com')
time.sleep(3)
browser.switch_to_window(browser.window_handles[0])  # 在第一个选项卡打开新浪页面
browser.get('https://www.sina.com.cn')
browser.close()
```

### 异常处理

```python
from selenium import webdriver
from selenium.common.exceptions import TimeoutException,NoSuchElementException,NoSuchFrameException

try:
    browser=webdriver.Chrome()
    browser.get('http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable')
    browser.switch_to.frame('iframssseResult')

except TimeoutException as e:
    print(e)
except NoSuchFrameException as e:
    print(e)
finally:
    browser.close()
```



# 并发处理

## 同步调用不使用并发

```python
# 同步调用
import requests

def parse_page(res):
    print('解析 %s' %(len(res)))

def get_page(url):
    print('下载 %s' %url)
    response=requests.get(url)
    if response.status_code == 200:
        return response.text

urls=['https://www.baidu.com/','http://www.sina.com.cn/','https://www.python.org']
for url in urls:
    res=get_page(url) #调用一个任务，就在原地等待任务结束拿到结果后才继续往后执行
    parse_page(res)
```

## 使用多进程(线程)

````python
#IO密集型程序应该用多线程
# 多进程或多线程
import requests
from threading import Thread,current_thread

def parse_page(res):
    print('%s 解析 %s' %(current_thread().getName(),len(res))) # current_thread().getName()获取当前线程的名字

def get_page(url,callback=parse_page):
    print('%s 下载 %s' %(current_thread().getName(),url))
    response=requests.get(url)
    if response.status_code == 200:
        callback(response.text)

if __name__ == '__main__':
    urls=['https://www.baidu.com/','http://www.sina.com.cn/','https://www.python.org']
    for url in urls:
        t=Thread(target=get_page,args=(url,))
        t.start()
# Thread-1 下载 https://www.baidu.com/
# Thread-2 下载 http://www.sina.com.cn/
# Thread-3 下载 https://www.python.org
# Thread-1 解析 2443
# Thread-2 解析 570061
# Thread-3 解析 48823
````

##  使用进程池(线程池)

py2:
​	进程池: from mutiprocessing import Pool
py3:
​	进程池: from concurrent.futures import ThreadPoolExecutor
​	线程池: from concurrent.futures import ProcessPoolExecutor

### py3

```python
#IO密集型程序应该用多线程，所以此时我们使用线程池
# 进程池或线程池：异步调用+回调机制
import requests
from threading import current_thread
from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor

def parse_page(res):
    res=res.result()    # 这个result()是线程池的回调函数中的参数是个对象需要使用result来得到线程的返回值
    print('%s 解析 %s' %(current_thread().getName(),len(res)))

def get_page(url):
    print('%s 下载 %s' %(current_thread().getName(),url))
    response=requests.get(url)
    if response.status_code == 200:
        return response.text

if __name__ == '__main__':
    urls=['https://www.baidu.com/','http://www.sina.com.cn/','https://www.python.org']

    pool=ThreadPoolExecutor(50)
    # pool=ProcessPoolExecutor(50)
    for url in urls:
        pool.submit(get_page,url).add_done_callback(parse_page)

    pool.shutdown(wait=True)
```

````python
import time
from threading import currentThread,get_ident      # currentThread  返回当前的线程变量  get_ident  返回当前的线程号,对比进程中的os.getpid
from concurrent.futures import ThreadPoolExecutor  # 帮助你启动线程池的类
from concurrent.futures import ProcessPoolExecutor  # 帮助你启动线程池的类

def func(i):
    time.sleep(1)
    print('in %s %s'%(i,currentThread()))
    return i**2

def back(fn):
    print(fn.result(),currentThread())

# map启动多线程任务
t = ThreadPoolExecutor(5)
t.map(func,range(20))
# 等价于
for i in range(20):
    t.submit(func,i)


# submit异步提交任务
t = ThreadPoolExecutor(5)
for i in range(20):
    t.submit(fn=func,)
t.shutdown()     # t.shutdown() 等价于 t.close()  t.join(),所以这个模块是高度封装的,帮我们省了很多事
print('main : ',currentThread())
# 起多少个线程池
    # 5*CPU的个数


# 获取任务结果
t = ThreadPoolExecutor(20)
ret_l = []
for i in range(20):
    ret = t.submit(func,i)
    ret_l.append(ret)
t.shutdown()
for ret in ret_l:
    print(ret.result())
print('main : ',currentThread())

# 回调函数
t = ThreadPoolExecutor(20)
for i in range(100):
    t.submit(func,i).add_done_callback(back)

# 回调函数(进程版)
import os
import time
from concurrent.futures import ProcessPoolExecutor  # 帮助你启动线程池的类

def func(i):
    time.sleep(1)
    print('in %s 子进程pid%s'%(i,os.getpid()))
    return i**2

def back(fn):
    print(fn.result(),os.getpid())  # 和mulitiprocessing 中的接收的参数就是返回值不同,这里的fn并不是返回值而是一个对象,需要使用fn,result()来得到返回值
if __name__ == '__main__':
    print('main : ',os.getpid())
    t = ProcessPoolExecutor(20)
    for i in range(100):
        t.submit(func,i).add_done_callback(back)

# multiprocessing模块自带进程池的
# threading模块是没有线程池的
# concurrent.futures 进程池 和 线程池
    # 高度封装
    # 进程池/线程池的统一的使用方式
# 创建线程池/进程池  ProcessPoolExecutor  ThreadPoolExecutor
# ret = t.submit(func,arg1,arg2....)  异步提交任务
# ret.result() 获取结果,如果要想实现异步效果,应该是使用列表
# map(func,iterable)
# shutdown 等价于 close+join 同步控制的\\
# add_done_callback 回调函数,在回调函数内接收的参数是一个对象,需要通过result来获取返回值
    # 进程池回调函数仍然在主进程中执行
    # 线程池中的回调函数在子线程中执行

# 线程池中使用回调函数
import os
import time
from threading import currentThread,get_ident
from concurrent.futures import ThreadPoolExecutor  # 帮助你启动线程池的类

def func(i):
    time.sleep(1)
    print('in %s 子线程pid %s'%(i,get_ident()))
    return i**2

def back(fn):
    print(fn.result(),'回调函数中的线程号',get_ident())
if __name__ == '__main__':
    print('main 主进程: ',os.getpid())
    t = ThreadPoolExecutor(20)
    for i in range(100):
        t.submit(func,i).add_done_callback(back)

````



### py2

````python
# 进程池
import os
import time
from multiprocessing import Pool
````

#### eg1:同步请求

````python
def wahaha():
    time.sleep(1)
    print(os.getpid())
    return True

if __name__ == '__main__':
    p = Pool(5)  # CPU的个数 或者 +1
    ret_l = []
    for i in range(20):
       ret = p.apply(func = wahaha)   # 同步的,不用
       print(ret)
````

#### eg2:异步提交,不获取返回值

````python
def wahaha():
    time.sleep(1)
    print(os.getpid())

if __name__ == '__main__':
    p = Pool(5)  # CPU的个数 或者 +1
    ret_l = []
    for i in range(20):
       ret = p.apply_async(func = wahaha) # async  异步的
       ret_l.append(ret)
    p.close()  # 关闭 不是进程池中的进程不工作了
               # 而是关闭了进程池,让任务不能再继续提交了
    p.join()   # 等待这个池中提交的任务都执行完
    # # 表示等待所有子进程中的代码都执行完 主进程才结束
````

#### eg3:异步提交,获取返回值,等待所有任务都执行完毕之后再统一获取结果

````python
def wahaha():
    time.sleep(1)
    print(os.getpid())
    return True

if __name__ == '__main__':
    p = Pool(5)  # CPU的个数 或者 +1
    ret_l = []
    for i in range(20):
       ret = p.apply_async(func = wahaha) # async  异步的
       ret_l.append(ret)
    p.close()  # 关闭 不是进程池中的进程不工作了
               # 而是关闭了进程池,让任务不能再继续提交了
    p.join()   # 等待这个池中提交的任务都执行完   p.close()  p.join() 一定是配合使用的
    for ret in ret_l:
        print(ret.get())

````

#### eg4:异步提交,获取返回值,一个任务执行完毕之后就可以获取到一个结果(顺序是按照提交任务的顺序)

````python
def wahaha():
    time.sleep(1)
    print(os.getpid())
    return True

if __name__ == '__main__':
    p = Pool(5)  # CPU的个数 或者 +1
    ret_l = []
    for i in range(20):
       ret = p.apply_async(func = wahaha) # async  异步的
       ret_l.append(ret)
    for ret in ret_l:
        print(ret.get())    # 因为要获取返回值,所以一定要逐个的等待子进程都结束,这里就不用写 p.close()  p.join() 了

````

异步的 apply_async
1.如果是异步的提交任务,那么任务提交之后进程池和主进程也异步了,
​    主进程不会自动等待进程池中的任务执行完毕
2.如果需要主进程等待,需要p.join
​    但是join的行为是依赖close
3.如果这个函数是有返回值的
​    也可以通过ret.get()来获取返回值
​    但是如果一边提交一遍获取返回值会让程序变成同步的
​    所以要想保留异步的效果,应该讲返回对象保存在列表里,所有任务提交完成之后再来取结果
​    这种方式也可以去掉join,来完成主进程的阻塞等待池中的任务执行完毕

普通进程  主进程和子进程异步,主进程代码结束,子进程不结束,子进程全部结束,主进程收尸
进程池    主进程和子进程,子进程之间全部异步(使用apply_async()),主进程代码结束,子进程结束(必须使用阻塞等待子进程执行完 1.time.sleep 2.子进程.join() 3.接收子进程的返回值 ret.get() 只有当子进程执行完后才会有返回值)

#### 回调函数

````python
import os
import time
import random
from multiprocessing import Pool

# 异步提交,获取返回值,从头到尾一个任务执行完毕之后就可以获取到一个结果
def wahaha(num):
    time.sleep(random.random())
    print('pid : ',os.getpid(),num)
    return num

def back(arg):
    print('call_back : ',os.getpid(),arg)

if __name__ == '__main__':
    print('主进程',os.getpid())
    p = Pool(5)  # CPU的个数 或者 +1
    for i in range(20):
       p.apply_async(func = wahaha,args=(i,),callback=back) # async  异步的
    p.close()
    p.join()

# 回调函数 _ 在主进程中执行
# 在发起任务的时候 指定callback参数
# 在每个进程执行完apply_async任务之后,返回值会直接作为参数传递给callback的函数,执行callback函数中的代码

````

### 协程

````python
# 使用协程减少IO操作带来的时间消耗
from gevent import monkey;monkey.patch_all()
import gevent
import time

def eat():
    print('吃')
    time.sleep(2)
    print('吃完了')

def play():
    print('玩儿')
    time.sleep(1)
    print('玩儿美了')

g1 = gevent.spawn(eat)
g2 = gevent.spawn(play)
gevent.joinall([g1,g2])  #等价于 g1.join()  +  g2,join()
# 吃
# 玩儿
# 玩儿美了
# 吃完了
    # gevent帮你做了切换,做切换是有条件的,遇到IO才切换
    # gevent不认识除了gevent这个模块内以外的IO操作
    # 使用join可以一直阻塞直到协程任务完成
# 帮助gevent来认识其他模块中的阻塞
    # from gevent import monkey;monkey.patch_all()写在其他模块导入之前

````

### 同时使用进程、线程、协程

````python
# from gevent import monkey;monkey.patch_all()  # 不能使用monkey 会把进程和线程里的sleep也识别到,后果很难预料
from concurrent.futures import ThreadPoolExecutor
from concurrent.futures import ProcessPoolExecutor
from threading import get_ident
import os
import time
import random
import gevent

def process_func(i):
    time.sleep(random.random())
    print('在子进程%s\t进程号%s:'% (i,os.getpid()))
    t = ThreadPoolExecutor(20)
    for x in range(20):
        t.submit(thread_func,i)
    t.shutdown()

def thread_func(i):
    time.sleep(random.random())
    print('在子进程%s\t进程号%s\t线程号%s:'% (i,os.getpid(),get_ident()))
    g_lst = []
    for y in range(500):
        g = gevent.spawn(xiecheng_func,y)
        g_lst.append(g)
    gevent.joinall(g_lst)   # 必须要阻塞,不然主线程执行完了协程还没有执行完就结束了

def xiecheng_func(i):
    gevent.sleep(random.random())
    print('在协程%s中'% i)



if __name__ == '__main__':
    '''开辟了一个进程池里面有5个进程,每个进程又开辟了一个线程池里面有20个线程,每个线程作为主线程里面有500个协程,这就是老师说的一般最大开辟的进程线程协程数'''
    print('主进程',os.getpid())
    p = ProcessPoolExecutor(5)

    for i in range(5):
        p.submit(process_func,i)    # 不能在主进程中实例化线程池,然后通过submit传参传过去,所以只能在子进程中实例化线程池了
    p.shutdown()
````



## 使用asyncio模块，可以帮我们检测IO（只能是网络IO），实现应用程序级别的切换

```python
# asycio基本使用,可以帮助我们检测网络IO
import asyncio

@asyncio.coroutine
def task(task_id,senconds):
    print('%s is start' %task_id)
    yield from asyncio.sleep(senconds) #只能检测网络IO,检测到IO后切换到其他任务执行
    print('%s is end' %task_id)

tasks=[task(task_id="任务1",senconds=3),task("任务2",2),task(task_id="任务3",senconds=1)]

loop=asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
loop.close()

# 任务2 is start  不知道为什么总是tasks列表中中间的那个先执行,调换了也是中间的先执行
# 任务1 is start
# 任务3 is start
# 任务3 is end
# 任务2 is end
# 任务1 is end
```



##  asyncio模块只能发tcp级别的请求，不能发http协议，因此，在我们需要发送http请求的时候，需要我们自定义http报头

```python
# asyncio模块只能发tcp级别的请求，不能发http协议，因此，在我们需要发送http请求的时候，需要我们自定义http报头
# asyncio+自定义http协议报头
import asyncio
import requests
import uuid
user_agent='Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0'

def parse_page(host,res):
    print('%s 解析结果 %s' %(host,len(res)))
    with open('%s.html' %(uuid.uuid1()),'wb') as f:
        # uuid.uuid1()　　基于MAC地址，时间戳，随机数来生成唯一的uuid，可以保证全球范围内的唯一性。详见博客:https://www.cnblogs.com/franknihao/p/7307224.html
        f.write(res)

@asyncio.coroutine
def get_page(host,port=80,url='/',callback=parse_page,ssl=False):   # 'www.baidu.com',url='/s?wd=美女',ssl=True
    print('下载 http://%s:%s%s' %(host,port,url))

    #步骤一（IO阻塞）：发起tcp链接，是阻塞操作，因此需要yield from
    if ssl:
        port=443
    recv,send=yield from asyncio.open_connection(host=host,port=443,ssl=ssl)
    # 什么是yield from呢？  详见博客:https://blog.csdn.net/qq_27825451/article/details/78847473
    #
    #      简单地说，yield from  generator 。实际上就是返回另外一个生成器的值。如下所示：
    #
    #
    # def generator2():
    # yield 'a'
    # yield 'b'
    # yield 'c'
    # yield from generator1() #yield from iterable本质上等于 for item in iterable: yield item的缩写版
    # yield from [11,22,33,44]
    # yield from (12,23,34)
    # yield from range(3)

    # 步骤二：封装http协议的报头，因为asyncio模块只能封装并发送tcp包，因此这一步需要我们自己封装http协议的包
    request_headers="""GET %s HTTP/1.0\r\nHost: %s\r\nUser-agent: %s\r\n\r\n""" %(url,host,user_agent)  # 请求头
    # requset_headers="""POST %s HTTP/1.0\r\nHost: %s\r\n\r\nname=egon&password=123""" % (url, host,)
    request_headers=request_headers.encode('utf-8')

    # 步骤三（IO阻塞）：发送http请求包
    send.write(request_headers)
    yield from send.drain()

    # 步骤四（IO阻塞）：接收响应头
    while True:
        line=yield from recv.readline()
        if line == b'\r\n':
            break
        print('%s Response headers：%s' %(host,line))

    # 步骤五（IO阻塞）：接收响应体
    text=yield from recv.read()

    # 步骤六：执行回调函数
    callback(host,text)

    # 步骤七：关闭套接字
    send.close() #没有recv.close()方法，因为是四次挥手断链接，双向链接的两端，一端发完数据后执行send.close()另外一端就被动地断开


if __name__ == '__main__':
    tasks=[
        get_page('www.baidu.com',url='/s?wd=美女',ssl=True),
        get_page('www.cnblogs.com',url='/',ssl=True),
    ]

    loop=asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(tasks))
    loop.close()
```

## 使用aiohttp模块，专门帮我们封装http报头，然后我们还需要用asyncio检测IO实现切换

```python
# 这段代码报错了,不知道是为什么
# asyncio+aiohttp
import aiohttp
import asyncio

@asyncio.coroutine
def get_page(url):
    print('GET:%s' %url)
    response=yield from aiohttp.request('GET',url)

    data=yield from response.read()

    print(url,data)
    response.close()
    return 1

tasks=[
    get_page('https://www.python.org/doc'),
    get_page('https://www.cnblogs.com/linhaifeng'),
    get_page('https://www.openstack.org')
]

loop=asyncio.get_event_loop()
results=loop.run_until_complete(asyncio.gather(*tasks))
loop.close()

print('=====>',results) #[1, 1, 1]
```

## 在上面的基础上将requests.get函数传给asyncio，就能够被检测了

```python
# asyncio+requests模块的方法
import requests
import asyncio

@asyncio.coroutine
def get_page(func,*args):   # requests.get,'https://www.python.org/doc'
    print('GET:%s' %args[0])
    loog=asyncio.get_event_loop()
    furture=loop.run_in_executor(None,func,*args)
    response=yield from furture

    print(response.url,len(response.text))
    return 1

tasks=[
    get_page(requests.get,'https://www.python.org/doc'),
    get_page(requests.get,'https://www.cnblogs.com/linhaifeng'),
    get_page(requests.get,'https://www.openstack.org')
]

loop=asyncio.get_event_loop()
results=loop.run_until_complete(asyncio.gather(*tasks))
loop.close()

print('=====>',results) #[1, 1, 1]
```

## gevent+requests使用协程

```python
# gevent+requests
from gevent import monkey;monkey.patch_all()
import gevent
import requests

def get_page(url):
    print('GET:%s' %url)
    response=requests.get(url)
    print(url,len(response.text))
    return 1

# g1=gevent.spawn(get_page,'https://www.python.org/doc')
# g2=gevent.spawn(get_page,'https://www.cnblogs.com/linhaifeng')
# g3=gevent.spawn(get_page,'https://www.openstack.org')
# gevent.joinall([g1,g2,g3,])
# print(g1.value,g2.value,g3.value) #拿到返回值


#协程池
from gevent.pool import Pool
pool=Pool(2)
g1=pool.spawn(get_page,'https://www.python.org/doc')
g2=pool.spawn(get_page,'https://www.cnblogs.com/linhaifeng')
g3=pool.spawn(get_page,'https://www.openstack.org')
gevent.joinall([g1,g2,g3,])
print(g1.value,g2.value,g3.value) #拿到返回值
```

## 使用封装了gevent+requests模块的grequests模块

```python
import grequests

request_list=[
    grequests.get('https://wwww.xxxx.org/doc1'),
    grequests.get('https://www.cnblogs.com/linhaifeng'),
    grequests.get('https://www.openstack.org')
]


##### 执行并获取响应列表 #####
# response_list = grequests.map(request_list)
# print(response_list)

##### 执行并获取响应列表（处理异常） #####
def exception_handler(request, exception):
    # print(request,exception)
    print("%s Request failed" %request.url)

response_list = grequests.map(request_list, exception_handler=exception_handler)
print(response_list,111)

# https://wwww.xxxx.org/doc1 Request failed
# [None, <Response [200]>, <Response [200]>] 111  reponse_list是个列表,元素是响应对象
```

##  twisted：是一个网络框架，其中一个功能是发送异步请求，检测IO并自动切换

```python
'''

#问题一：error: Microsoft Visual C++ 14.0 is required. Get it with "Microsoft Visual C++ Build Tools": http://landinghub.visualstudio.com/visual-cpp-build-tools
https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
pip3 install C:\Users\Administrator\Downloads\Twisted-17.9.0-cp36-cp36m-win_amd64.whl
pip3 install twisted

#问题二：ModuleNotFoundError: No module named 'win32api'
https://sourceforge.net/projects/pywin32/files/pywin32/

#问题三：openssl
pip3 install pyopenssl
'''

#twisted基本用法
from twisted.web.client import getPage,defer
from twisted.internet import reactor

def all_done(arg):
    # print(arg)
    reactor.stop()

def callback(res):
    print(res)
    return 1

defer_list=[]
urls=[
    'http://www.baidu.com',
    'http://www.bing.com',
    'https://www.python.org',
]
for url in urls:
    obj=getPage(url.encode('utf=-8'),)
    obj.addCallback(callback)
    defer_list.append(obj)

defer.DeferredList(defer_list).addBoth(all_done)

reactor.run()




#twisted的getPage的详细用法
from twisted.internet import reactor
from twisted.web.client import getPage
import urllib.parse


def one_done(arg):
    print(arg)
    reactor.stop()

post_data = urllib.parse.urlencode({'check_data': 'adf'})
post_data = bytes(post_data, encoding='utf8')
headers = {b'Content-Type': b'application/x-www-form-urlencoded'}
response = getPage(bytes('http://dig.chouti.com/login', encoding='utf8'),
                   method=bytes('POST', encoding='utf8'),
                   postdata=post_data,
                   cookies={},
                   headers=headers)
response.addBoth(one_done)

reactor.run()
```

## tornado

```python
from tornado.httpclient import AsyncHTTPClient
from tornado.httpclient import HTTPRequest
from tornado import ioloop


def handle_response(response):
    """
    处理返回值内容（需要维护计数器，来停止IO循环），调用 ioloop.IOLoop.current().stop()
    :param response:
    :return:
    """
    if response.error:
        print("Error:", response.error)
    else:
        print(response.body)


def func():
    url_list = [
        'http://www.baidu.com',
        'http://www.bing.com',
    ]
    for url in url_list:
        print(url)
        http_client = AsyncHTTPClient()
        http_client.fetch(HTTPRequest(url), handle_response)


ioloop.IOLoop.current().add_callback(func)
ioloop.IOLoop.current().start()




#发现上例在所有任务都完毕后也不能正常结束，为了解决该问题，让我们来加上计数器
from tornado.httpclient import AsyncHTTPClient
from tornado.httpclient import HTTPRequest
from tornado import ioloop

count=0

def handle_response(response):
    """
    处理返回值内容（需要维护计数器，来停止IO循环），调用 ioloop.IOLoop.current().stop()
    :param response:
    :return:
    """
    if response.error:
        print("Error:", response.error)
    else:
        print(len(response.body))

    global count
    count-=1 #完成一次回调，计数减1
    if count == 0:
        ioloop.IOLoop.current().stop() 

def func():
    url_list = [
        'http://www.baidu.com',
        'http://www.bing.com',
    ]

    global count
    for url in url_list:
        print(url)
        http_client = AsyncHTTPClient()
        http_client.fetch(HTTPRequest(url), handle_response)
        count+=1 #计数加1

ioloop.IOLoop.current().add_callback(func)
ioloop.IOLoop.current().start()
```



# celery

[基于redis的celery](https://www.cnblogs.com/perfey/p/10216434.html)

# linux

## 常用命令



# 数据结构

[数据结构](https://www.cnblogs.com/perfey/p/10121610.html)

# 算法

[递归、排序](https://www.cnblogs.com/perfey/p/10200377.html)







# 爬虫技巧

- 使用mitmdump抓包时,好像是和charles冲突,所以需要指定端口 mitmdump -p 8888, 指定python脚本是-s XXX.py
  - 这个明白了,因为我是使用电脑开热点,然后手机连接的charles的默认端口是8888,启用charles后默认的代理是 电脑ip + 端口(8888),而使用mitmdump后只有使用此端口才能使用电脑的代理
  - 当电脑和手机都连接路由器后,在连接的wifi的高级设置中,设置代理电脑的ip 和端口(charles的默认端口是8888,mitmdump的默认端口是8080),这样才可以使用代理电脑连网上网
  - 注意代理电脑的防火墙要关闭,不然可能不成功
- 使用csv写文件会出现空行,with open("douyin.csv", 'w',newline='') as csvfile ,在open中加入参数newline=''就可以了
- ajax动态渲染也是使用requests访问ajax请求的ip,返回json 数据
- selenium可以在运行时手动输入验证码
- 可以使用整站下载工具,先下载好需要爬的网站,然后再下载好的网站进行解析,这样就没有封ip的问题了,当然这只适用于不用登陆的网站
